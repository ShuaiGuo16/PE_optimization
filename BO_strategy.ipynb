{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28915f4-c2d8-4d5f-8fef-dbde6dc3b7be",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, we investigate the Bayesian optimization strategy for PE optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ee3f20-d803-4e0d-b4b5-725515ae4b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shuai\\anaconda3\\envs\\SciML\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Shuai\\anaconda3\\envs\\SciML\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Shuai\\anaconda3\\envs\\SciML\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from scipy.stats import qmc, norm\n",
    "import gpflow\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "import utility\n",
    "from two_sources import thermal_distribution_maxT\n",
    "from manager import region_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17763d-9153-4c5b-90f3-6e943153ecfb",
   "metadata": {},
   "source": [
    "### 0. Setup necessary constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d867d3e8-6b04-476f-ac0e-866f4ae8d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tjmax = 175\n",
    "\n",
    "# Bounds: d, b, L, c, L_duct, n\n",
    "lb = np.array([5e-3, 73.7e-3, 127.2e-3, 10e-3, 20e-3, 10])\n",
    "ub = np.array([30e-3, 307e-3, 530e-3, 39e-3, 50e-3, 50])\n",
    "Data = (25, 50e-3, 65e-3, 61.4e-3, 106e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925d3ba-74ab-47d6-a3d3-4ceb9d96e882",
   "metadata": {},
   "source": [
    "### 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b444d479-9873-4e7a-9db1-d767a7fadd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pool: 939\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./dataset/passive_learning_train.csv')\n",
    "df_train.columns = ['Q1', 'Q2', 'd', 'b', 'L', 'c', 'L_duct', 'n', 't', 'xc1', 'yc1', 'xc2', 'yc2', 'Tc', 'Tj', 'w']\n",
    "print(f\"Training pool: {df_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b25128-070e-46a6-a9d8-f80ece8ea379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate pool: 470070\n"
     ]
    }
   ],
   "source": [
    "df_candidates = pd.read_csv('./dataset/candidates.csv')\n",
    "df_candidates.columns = ['d', 'b', 'L', 'c', 'L_duct', 'n', 't', 'xc1', 'yc1', 'xc2', 'yc2']\n",
    "print(f\"Candidate pool: {df_candidates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df64db09-e273-4169-bbc6-9cdc443bd240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset: 5655\n"
     ]
    }
   ],
   "source": [
    "# Dedicated testing set\n",
    "df_test = pd.read_csv('./dataset/test.csv')\n",
    "df_test.columns = ['Q1', 'Q2', 'd', 'b', 'L', 'c', 'L_duct', 'n', 't', 'xc1', 'yc1', 'xc2', 'yc2', 'Tc', 'Tj', 'w']\n",
    "print(f\"Testing dataset: {df_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f99696-6c22-4a08-aed1-574c52ba4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract train data\n",
    "X, _, y, _ = utility.create_samples(df_train, len(df_train))\n",
    "X_test, _, y_test, _ = utility.create_samples(df_test, len(df_test))\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54265e-881e-473d-9a58-0186ff3be532",
   "metadata": {},
   "source": [
    "### 2. BO iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46677b4c-7b84-4a2a-91a4-33f490390c79",
   "metadata": {},
   "source": [
    "#### 2.1 Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a002500-16b6-44e5-8ce6-af49cbae66fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 1-th optimization:\n",
      "Performing 2-th optimization:\n",
      "Performing 3-th optimization:\n",
      "Performing 4-th optimization:\n",
      "Performing 5-th optimization:\n",
      "Performing 6-th optimization:\n",
      "Performing 7-th optimization:\n",
      "Performing 8-th optimization:\n",
      "Performing 9-th optimization:\n",
      "Performing 10-th optimization:\n",
      "CPU times: total: 19min 41s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train = copy.deepcopy(X)\n",
    "X_train_scaled = copy.deepcopy(X_scaled)\n",
    "y_train = copy.deepcopy(y)\n",
    "\n",
    "GP = utility.fit(X_train_scaled, y_train, n_restarts=10, trainable=True, verbose=True)\n",
    "\n",
    "# Load GP model\n",
    "# with open('./model/PL_model_params_939.pickle', 'rb') as handle:\n",
    "#     GP_params = pickle.load(handle)\n",
    "# GP = utility.load_GP_model(GP_params, X, y)\n",
    "\n",
    "model = copy.deepcopy(GP)\n",
    "init_lengthscales = model.kernel.lengthscales.numpy().reshape(1, -1)\n",
    "init_variance = model.kernel.variance.numpy().flatten()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d6fe8e7-62ec-41f8-bac9-61cebfd6e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 8.7686 / data std: 32.8753\n",
      "Max Error: 99.1912\n",
      "Max Percentage Error: 40.15\n",
      "Mean Percentage Error: 4.92\n",
      "Brier score: 0.01932\n"
     ]
    }
   ],
   "source": [
    "X_test_norm = scaler.transform(X_test)\n",
    "f_mean, f_var = model.predict_f(X_test_norm, full_cov=False)\n",
    "y_prob = norm.cdf(175, loc=f_mean, scale=np.sqrt(f_var))\n",
    "label = np.where(y_test > 175, 1, 0)\n",
    "brier_score = brier_score_loss(label, 1-y_prob)\n",
    "    \n",
    "rmse, max_e, max_per, _, mean_per = utility.evaluate_model(y_test, f_mean.numpy().flatten())\n",
    "print(f\"RMSE: {rmse:.4f} / data std: {np.std(y_test):.4f}\")\n",
    "print(f\"Max Error: {max_e:.4f}\")\n",
    "print(f\"Max Percentage Error: {max_per:.2f}\")\n",
    "print(f\"Mean Percentage Error: {mean_per:.2f}\")\n",
    "print(f\"Brier score: {brier_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07cc4c3-d6f4-4529-a00f-484a1252f91e",
   "metadata": {},
   "source": [
    "#### 2.2 Given Q1-Q2 specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65c12f-9c11-4585-9ab7-a56760b16f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose dataset\n",
    "Q1, Q2 = 300, 200\n",
    "Q1_array, Q2_array = Q1*np.ones((df_candidates.shape[0], 1)), Q2*np.ones((df_candidates.shape[0], 1))\n",
    "X_candidates = df_candidates.to_numpy()\n",
    "X_candidates = np.hstack((Q1_array, Q2_array, X_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72ba6e-42f9-480b-8085-730059f4850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_mean, f_var = utility.GP_predict_candidates(model, X_candidates, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95de24ba-1e58-4b9c-a0f5-18cf8ea38746",
   "metadata": {},
   "source": [
    "**Note that indeed the inference time highly depends on the training data size.**\n",
    "\n",
    "939 ==> 48.3s\n",
    "\n",
    "200 ==> 5.14s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91356249-7d49-468f-b91d-df1f582c86ab",
   "metadata": {},
   "source": [
    "#### 2.3 BO setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff0ff4-0ca2-4c33-a0d7-c0ee64696807",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 1-Select promising candidates from the pool\n",
    "n_batch = 10\n",
    "acq, indices = utility.acquisition(None, f_mean, f_var, X_candidates, scaler, \n",
    "                                   Tjmax, batch_mode=True, batch_size=n_batch)\n",
    "print(\"Candidates selected!\")\n",
    "\n",
    "managers = []\n",
    "for index in indices:\n",
    "\n",
    "    # Enrich dataset\n",
    "    X_train = np.vstack((X_train, X_candidates[index]))\n",
    "    y, _ = thermal_distribution_maxT(X_candidates[index], Data)\n",
    "    y_train = np.append(y_train, np.array(y))\n",
    "\n",
    "    # Create managers\n",
    "    manager = region_manager(model, n_batch, Data, Tjmax, X_candidates[index], Q1, Q2)\n",
    "    managers.append(manager)\n",
    "\n",
    "    # Update weight\n",
    "    if y <= manager.Tjmax:\n",
    "        w = utility.evaluate_weight(X_candidates[index].reshape(1, -1))[0]\n",
    "    else:\n",
    "        w = np.inf\n",
    "        \n",
    "    manager.init_weight(w)\n",
    "\n",
    "print(\"Managers created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e45ff-ee56-4689-8216-09f8e7c660eb",
   "metadata": {},
   "source": [
    "#### 2.4 Branch & Bound algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f4cda-cd69-4c17-8b6e-2dd9931637cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "counter = 0\n",
    "status_list = [False] * n_batch\n",
    "design_list, weight_list = [], []\n",
    "\n",
    "while np.sum(status_list) < n_batch:\n",
    "    counter += 1\n",
    "    print(f\"Start {counter}-th iteration:\")\n",
    "\n",
    "    # Update global GP model\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    model = utility.fit(X_train_scaled, y_train, n_restarts=1, init_lengthscales=init_lengthscales.flatten(), \n",
    "                        init_variance=init_variance, trainable=False, verbose=False)\n",
    "    print(\"Global model updated!\")\n",
    "    \n",
    "    for i, (manager, index) in enumerate(zip(managers, indices)):\n",
    "        print(f\"Process {i+1}/{n_batch} manager:\")\n",
    "        print(\"Updating manager model:\")\n",
    "        manager.update_model(model)\n",
    "        \n",
    "        if not manager.converge_flag:\n",
    "            X, y = manager.branch_and_bound(lb, ub, scaler, candidate_num=5000)\n",
    "            X_train = np.vstack((X_train, X))\n",
    "            y_train = np.append(y_train, np.array(y))\n",
    "    \n",
    "            # 5-Check conditions\n",
    "            design, weight, converge_flag = manager.check_converge()   \n",
    "            status_list[i] = converge_flag\n",
    "\n",
    "            if design is not None:\n",
    "                design_list.append(design)\n",
    "                weight_list.append(weight)\n",
    "                print(f\"{i+1}/{n_batch} manager done!\")\n",
    "        else:\n",
    "            print(f\"Skip {i+1}/{n_batch} manager.\")\n",
    "\n",
    "print(f\"All managers done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee411b9-8541-46fc-aa64-1e6ae73c1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "best_weight = np.min(weight_list)\n",
    "best_design = design_list[np.argmin(weight_list)]\n",
    "Tmax, _ = thermal_distribution_maxT(best_design, Data)\n",
    "print(f\"Best weight: {best_weight}\")\n",
    "print(f\"Tmax: {Tmax}\")\n",
    "\n",
    "columns = ['Q1', 'Q2', 'd', 'b', 'L', 'c', 'L_duct', 'n', 't', 'xc1', 'yc1', 'xc2', 'yc2']\n",
    "df = pd.DataFrame({'optimized': best_design})\n",
    "df.index = columns\n",
    "df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda2090e-ad4e-4a53-b26a-2b2686285bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
