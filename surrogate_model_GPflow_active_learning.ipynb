{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979c2810",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, we attempt to build surrogate models for predicting the thermal resistance. \n",
    "\n",
    "Here, we aim to train GP adaptively to approximate the Tjmax=175 limit state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6f4ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CHSHGUO\\Anaconda3\\envs\\SciML\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CHSHGUO\\Anaconda3\\envs\\SciML\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\CHSHGUO\\Anaconda3\\envs\\SciML\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import qmc, norm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, max_error, brier_score_loss\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94f231",
   "metadata": {},
   "source": [
    "### 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a467ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool: 9421\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./Dataset/TcTj_train.csv', header=None)\n",
    "df.columns = ['Q1', 'Q2', 'd', 'b', 'L', 'c', 'L_duct', 'n', 't', 'xc1', 'yc1', 'xc2', 'yc2', 'Tc', 'Tj', 'w']\n",
    "print(f\"Pool: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd780065-62c1-49ef-b35f-a9c7c48ca451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered pol: 9386\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers\n",
    "df = df[df.Tj<250].reset_index(drop=True)\n",
    "print(f\"Filtered pol: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef413e0d-18f2-4e05-a28e-2ec2b30e4d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered testing pol: 9375\n"
     ]
    }
   ],
   "source": [
    "# Dedicated testing set\n",
    "df_test = pd.read_csv('./Dataset/TcTj_test.csv', header=None)\n",
    "df_test.columns = ['Q1', 'Q2', 'd', 'b', 'L', 'c', 'L_duct', 'n', 't', 'xc1', 'yc1', 'xc2', 'yc2', 'Tc', 'Tj', 'w']\n",
    "\n",
    "# Remove outliers\n",
    "df_test = df_test[df_test.Tj<250].reset_index(drop=True)\n",
    "print(f\"Filtered testing pol: {df_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae4178b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(df, train_num):\n",
    "   \n",
    "    # Create dataset\n",
    "    X = df.iloc[:, :-3].to_numpy()\n",
    "    y = df.iloc[:, -2].to_numpy()\n",
    "    \n",
    "    # Train-test split\n",
    "    if train_num < len(df):\n",
    "        test_size = 1-train_num/len(df)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    else:\n",
    "        X_train, y_train = X, y\n",
    "        X_test, y_test = None, None\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7786479d-29b2-41fa-a8cc-43881cf99208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X, _, y, _ = create_samples(df, 9000)\n",
    "X_test, _, y_test, _ = create_samples(df_test, 9000)\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055a43e",
   "metadata": {},
   "source": [
    "### 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58670404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"This function is used for evaluating the ML models performance.\"\"\"\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    max_e = max_error(y_true, y_pred)\n",
    "    \n",
    "    percentage = np.abs(y_true-y_pred)/y_true\n",
    "    max_percentage = np.max(percentage)*100\n",
    "    max_percentage_loc = np.argmax(percentage)\n",
    "    mean_percentage = np.mean(percentage)*100\n",
    "    \n",
    "    return rmse, max_e, max_percentage, max_percentage_loc, mean_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c77e600-5d94-482a-8c7f-d61acd0d094a",
   "metadata": {},
   "source": [
    "#### GPflow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9430bc04-3ad5-4c97-abf3-29fd8ebb0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_length_scales(dim, n_restarts, initial_guess=None):\n",
    "    \n",
    "    # Random initial params\n",
    "    lb, ub = -2, 2\n",
    "    lhd = qmc.LatinHypercube(d=dim, seed=42).random(n_restarts)\n",
    "    lhd = (ub-lb)*lhd + lb\n",
    "    length_scales = 10**lhd\n",
    "\n",
    "    # Informed initial guess\n",
    "    if initial_guess is not None:\n",
    "        length_scales = np.vstack((length_scales, initial_guess))\n",
    "\n",
    "    return length_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d895448b-23c8-4cf0-abf0-f8bdc8903a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, n_restarts=20, init_lengthscales=None, init_variance=None, trainable=True, verbose=True):\n",
    "    models = []\n",
    "    log_likelihoods = []\n",
    "    \n",
    "    # Generate initial guesses for length scale\n",
    "    length_scales = init_length_scales(X.shape[1], n_restarts, init_lengthscales)\n",
    "    if init_variance is None:\n",
    "        variance=np.var(y_train)\n",
    "    else:\n",
    "        variance=init_variance\n",
    "\n",
    "    if not trainable:\n",
    "        model = gpflow.models.GPR(\n",
    "            (X, y.reshape(-1, 1)),\n",
    "            kernel=gpflow.kernels.SquaredExponential(variance=variance, lengthscales=init_lengthscales),\n",
    "            mean_function=gpflow.functions.Polynomial(0),\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    else:\n",
    "        with tf.device(\"CPU:0\"):\n",
    "            \n",
    "            for i, init in enumerate(length_scales):\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Performing {i+1}-th optimization:\")\n",
    "    \n",
    "                # Set up the model\n",
    "                kernel = gpflow.kernels.SquaredExponential(variance=variance, lengthscales=init)\n",
    "                model = gpflow.models.GPR(\n",
    "                    (X, y.reshape(-1, 1)),\n",
    "                    kernel=kernel,\n",
    "                    mean_function=gpflow.functions.Polynomial(0),\n",
    "                )\n",
    "    \n",
    "                opt = gpflow.optimizers.Scipy()\n",
    "                opt.minimize(model.training_loss, model.trainable_variables, options=dict(maxiter=100))\n",
    "    \n",
    "                models.append(model)\n",
    "                log_likelihoods.append(model.log_marginal_likelihood().numpy())\n",
    "    \n",
    "        # Select the model with the highest log-marginal likelihood\n",
    "        best_model_index = np.argmax(log_likelihoods)\n",
    "        best_model = models[best_model_index]\n",
    "\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d54367-b787-405b-9e85-d859152f8c36",
   "metadata": {},
   "source": [
    "#### Active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2cdd6f-ae56-49f0-a719-8f7d021b06d1",
   "metadata": {},
   "source": [
    "Select diverse batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10cc91c9-a4c3-4bc1-b038-1cd44bc499b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_diverse_batch(samples, acq, batch_size=5, pre_filter=False):\n",
    "    \n",
    "    if pre_filter:\n",
    "        thred = np.quantile(acq, pre_filter)\n",
    "        filtered_indices = np.arange(len(samples))[acq>thred]\n",
    "        samples = samples[acq>thred]\n",
    "        acq = acq[acq>thred]\n",
    "    \n",
    "    else:\n",
    "        filtered_indices = np.arange(len(samples))\n",
    "        \n",
    "    # Perform weighted K-means clustering on the samples\n",
    "    kmeans = KMeans(n_clusters=batch_size, n_init=10, random_state=0).fit(samples, sample_weight=acq)\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Find the highest acquisition value sample in each cluster\n",
    "    selected_indices = []\n",
    "    for cluster_idx in range(batch_size):\n",
    "        cluster_indices = np.where(cluster_labels == cluster_idx)[0]\n",
    "        cluster_acquisition_values = acq[cluster_indices]\n",
    "        best_index_in_cluster = cluster_indices[np.argmax(cluster_acquisition_values)]\n",
    "        selected_indices.append(best_index_in_cluster)\n",
    "\n",
    "    return filtered_indices[selected_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b335b-7437-446a-be22-3c9dab1da9cc",
   "metadata": {},
   "source": [
    "Acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50ea9b55-8617-471a-b68b-3dfe8d3fbdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquisition(model, candidate, limit_state_value=0, batch_mode=False, batch_size=None):\n",
    "\n",
    "    # Compute prediction variance\n",
    "    f_mean, f_var = model.predict_f(candidate, full_cov=False)\n",
    "    f_mean = f_mean.numpy().flatten()\n",
    "    f_var = f_var.numpy().flatten()\n",
    "\n",
    "    # Calculate U values\n",
    "    U_values = np.abs(f_mean-limit_state_value)/np.sqrt(f_var)\n",
    "\n",
    "    # Sample selection\n",
    "    if batch_mode:\n",
    "        # Batch selection mode\n",
    "        U_normalied = MinMaxScaler().fit_transform(1/U_values.reshape(-1, 1))\n",
    "        indices = select_diverse_batch(candidate, U_normalied.flatten(), batch_size=batch_size)\n",
    "    \n",
    "    else:\n",
    "        # Single point selection mode\n",
    "        indices = np.argmin(U_values)\n",
    "\n",
    "    return U_values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c143794-ddc6-4e7f-b616-ebb330174e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_creator(X, y, sample_num, sampling_scheme='LHS'):\n",
    "    \n",
    "    # Create virtual samples\n",
    "    if sampling_scheme == 'LHS':\n",
    "        raw_virtual_samples = qmc.LatinHypercube(d=X.shape[1]).random(n=sample_num)\n",
    "    elif sampling_scheme == 'Halton':\n",
    "        raw_virtual_samples = qmc.Halton(d=X.shape[1]).random(sample_num)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid sampling scheme: {sampling_scheme}\")\n",
    "\n",
    "    # Dataset statistics\n",
    "    X_scaled = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "    # Find closest real samples\n",
    "    sample_finder = NearestNeighbors(n_neighbors=1).fit(X_scaled)\n",
    "    _, indices = sample_finder.kneighbors(raw_virtual_samples)\n",
    "    \n",
    "    # Drop duplicates\n",
    "    indices = np.unique(indices)\n",
    "\n",
    "    # Select samples\n",
    "    selected_X = X[indices.flatten()]\n",
    "    selected_y = y[indices.flatten()]\n",
    "\n",
    "    # Identify remaining samples\n",
    "    all_indices = np.arange(X.shape[0])\n",
    "    remaining_indices = np.setdiff1d(all_indices, indices)\n",
    "    remaining_X = X[remaining_indices]\n",
    "    remaining_y = y[remaining_indices]\n",
    "  \n",
    "    return selected_X, selected_y, remaining_X, remaining_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7af04a1c-2a3a-439f-9bc7-30c982cf05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_assessment(model, X_test, y_test, limit_state):\n",
    "    f_mean, f_var = model.predict_f(X_test, full_cov=False)\n",
    "    y_prob = norm.cdf(limit_state, loc=f_mean, scale=np.sqrt(f_var))\n",
    "    label = np.where(y_test > limit_state, 1, 0)\n",
    "    brier_score = brier_score_loss(label, 1-y_prob)\n",
    "    return brier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72a411a1-c83d-443b-b0e3-3ed1db4c0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial samples \n",
    "ini_sample_num = 300\n",
    "X_train, y_train, X_pool, y_pool = sample_creator(X, y, ini_sample_num, sampling_scheme='Halton')\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c43c9ad-f0dc-4473-bcab-c97cb96a5a72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start 1th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 1: test brier score==>0.01497/0.01039, U==>0.0002/1.65, index==>[ 655 3142  801   66 4568 1605 6252  880 7363 5351]\n",
      "Start 2th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 2: test brier score==>0.01427/0.01039, U==>0.0142/1.65, index==>[4185 2733 6231 4083 3577 6412 2490 1304 6560 2103]\n",
      "Start 3th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 3: test brier score==>0.01544/0.01039, U==>0.0115/1.65, index==>[1662 3726 1472 1948 1151 4225 6495 5165 4585 3840]\n",
      "Start 4th learning iteration:\n",
      "Bad fitting. Refit the data:\n",
      "Good fitting. Proceed:\n",
      "Iter 4: test brier score==>0.01487/0.01039, U==>0.0101/1.65, index==>[1495  437 1235 3561 8579 2957 4755 5338 4212 6924]\n",
      "Start 5th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 5: test brier score==>0.01482/0.01039, U==>0.0049/1.65, index==>[3833  998 7383 5034 7205 1377 5515 4988 6017 5614]\n",
      "Start 6th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 6: test brier score==>0.01421/0.01039, U==>0.0303/1.65, index==>[5182 6807 4744 3549 8658 6602 2069  419   12 4305]\n",
      "Start 7th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 7: test brier score==>0.01372/0.01039, U==>0.0593/1.65, index==>[ 611 5590 2323 6336 4390 6983  313 7505 7446 6031]\n",
      "Start 8th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 8: test brier score==>0.01294/0.01039, U==>0.0000/1.65, index==>[1383 1917 5419 4659 8527 1275 1526 8164 7867 5301]\n",
      "Start 9th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 9: test brier score==>0.01246/0.01039, U==>0.0095/1.65, index==>[5351 7174  496 4684  773  810 6869  997 2480 5704]\n",
      "Start 10th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 10: test brier score==>0.01184/0.01039, U==>0.0119/1.65, index==>[2901 4460 4121 2691 7753 6441 1998 5628 6450 5391]\n",
      "Start 11th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 11: test brier score==>0.01236/0.01039, U==>0.0000/1.65, index==>[7022 1249 2985 6916 6945 2214 5657 5353 5534 5966]\n",
      "Start 12th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 12: test brier score==>0.01140/0.01039, U==>0.0338/1.65, index==>[2723 7663 1236 6981 5189 4769 2623 3613 4522 7382]\n",
      "Start 13th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 13: test brier score==>0.01115/0.01039, U==>0.0073/1.65, index==>[4866 1284 5792 8186 6045 7928 5088 8183 7941 2450]\n",
      "Start 14th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 14: test brier score==>0.01088/0.01039, U==>0.0042/1.65, index==>[ 537 6956 8054 8412 1247 6301 4604 1947 8488 7393]\n",
      "Start 15th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 15: test brier score==>0.01078/0.01039, U==>0.0481/1.65, index==>[1537 6000 2592 5798  667 3666 1318 3248 5092 5034]\n",
      "Start 16th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 16: test brier score==>0.01076/0.01039, U==>0.0793/1.65, index==>[8065 6374  841 5019 3932 4806 1832 2158 2721 4342]\n",
      "Start 17th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 17: test brier score==>0.01059/0.01039, U==>0.4837/1.65, index==>[6125  743 2376 4686 5041  963 4771 1200  323 8288]\n",
      "Start 18th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 18: test brier score==>0.01047/0.01039, U==>0.4323/1.65, index==>[6582 3839 1513 7991 8098 1093 5392 3695 8233  300]\n",
      "Start 19th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 19: test brier score==>0.01052/0.01039, U==>0.0334/1.65, index==>[7406 7334 1891  895 8143 2772 2381 2367  473 7880]\n",
      "Start 20th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 20: test brier score==>0.01034/0.01039, U==>0.4072/1.65, index==>[8499 1496 2470 4818 3230 3525  990 2659 5779 5248]\n",
      "Start 21th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 21: test brier score==>0.01018/0.01039, U==>0.3741/1.65, index==>[8071  298 6204 1368 7380  400 3411 7499 2209  775]\n",
      "Start 22th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 22: test brier score==>0.01030/0.01039, U==>0.0457/1.65, index==>[ 451  481 7389 3510 1606  490 5701 8062 2986  262]\n",
      "Start 23th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 23: test brier score==>0.01017/0.01039, U==>0.0081/1.65, index==>[1407 1150  503 4285 2032 6915  484 7055 7695 8134]\n",
      "Start 24th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 24: test brier score==>0.01012/0.01039, U==>0.0241/1.65, index==>[6524 7165 7250 8424 3395 2573 8109 4266 4405 6911]\n",
      "Start 25th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 25: test brier score==>0.01048/0.01039, U==>0.1543/1.65, index==>[7045 5211 7750 5564 6235 7752 8189 5156 4208 2109]\n",
      "Start 26th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 26: test brier score==>0.01036/0.01039, U==>0.1421/1.65, index==>[4621 1273  824 6864 1301 6573 7429 3395 2778 1466]\n",
      "Start 27th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 27: test brier score==>0.01004/0.01039, U==>0.0602/1.65, index==>[6310  514 6485 4225   48  915 7670 7436 6330 8246]\n",
      "Start 28th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 28: test brier score==>0.01025/0.01039, U==>0.4396/1.65, index==>[5714 5471 7685   70 2263   65 6521  447 7202  134]\n",
      "Start 29th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 29: test brier score==>0.01012/0.01039, U==>1.0154/1.65, index==>[7621 7995 5153 7496 1916 3740 4534 6575 2600 8158]\n",
      "Start 30th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 30: test brier score==>0.01003/0.01039, U==>1.2941/1.65, index==>[2939   48  570 1530 2172 4116 3983 6067 5463 3030]\n",
      "Start 31th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 31: test brier score==>0.00975/0.01039, U==>0.3797/1.65, index==>[5281 5727 3861 8018 7031 7719 4175 3169 2697 1272]\n",
      "Start 32th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 32: test brier score==>0.00980/0.01039, U==>0.7356/1.65, index==>[5083 1435 6792 3369 7770 8062 6885 7202 2237 7294]\n",
      "Start 33th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 33: test brier score==>0.01002/0.01039, U==>1.2937/1.65, index==>[4642 4213 1618 4813  582 4244 7662 7812 2853 1035]\n",
      "Start 34th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 34: test brier score==>0.00968/0.01039, U==>0.8971/1.65, index==>[5011 4074 2829 1726 5647 6224 1975 7722  518 7336]\n",
      "Start 35th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 35: test brier score==>0.00953/0.01039, U==>0.6851/1.65, index==>[4989 3802 3823 1159 6827 6853 2186 5667 3901 1808]\n",
      "Start 36th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 36: test brier score==>0.00939/0.01039, U==>0.9266/1.65, index==>[ 358 8268   76 2864 4519  182 7452 2468 1508 5981]\n",
      "Start 37th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 37: test brier score==>0.00937/0.01039, U==>0.9122/1.65, index==>[7046 4798 6313 5943  704 4661 3155 6537 4536 7042]\n",
      "Start 38th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 38: test brier score==>0.00947/0.01039, U==>0.8808/1.65, index==>[6289 3238 3341   58 7024 7808 3201 8085  303 2612]\n",
      "Start 39th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 39: test brier score==>0.00937/0.01039, U==>1.2824/1.65, index==>[7613 7174  701 3275 6448 7052 5385 3492 2549 2122]\n",
      "Start 40th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 40: test brier score==>0.00954/0.01039, U==>1.7025/1.65, index==>[7836 3505  510 2222 4461 6952 7813 2849 6141  653]\n",
      "Start 41th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 41: test brier score==>0.00932/0.01039, U==>1.6706/1.65, index==>[7571 4858 4307 4351 6745 4331 5777 3842 3402 6014]\n",
      "Start 42th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 42: test brier score==>0.00932/0.01039, U==>1.8577/1.65, index==>[3951 7321  891 4734 7029 2363 6494 3511 4370 5941]\n",
      "Start 43th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 43: test brier score==>0.00960/0.01039, U==>1.3956/1.65, index==>[4457 2765  368 6105 6272 4367 7742 7375 5217 7804]\n",
      "Start 44th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 44: test brier score==>0.00917/0.01039, U==>1.8026/1.65, index==>[7977 3370 7287 6139  725 2368 2366 6875 2617 7346]\n",
      "Start 45th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 45: test brier score==>0.00895/0.01039, U==>1.2224/1.65, index==>[5775 3022 6465 2413 7600 4023 8176 1361 6347 6155]\n",
      "Start 46th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 46: test brier score==>0.00899/0.01039, U==>1.8035/1.65, index==>[ 391 4586 1107 7575 2242 6884 7240  700 3489 5373]\n",
      "Start 47th learning iteration:\n",
      "Good fitting. Proceed:\n",
      "Iter 47: test brier score==>0.00886/0.01039, U==>2.0551/1.65, index==>[5672 7751 3643 6670  182  866 1172 2522 8056 6887]\n"
     ]
    }
   ],
   "source": [
    "n_iter = 100\n",
    "U_hist, test_brier_scores = [], []\n",
    "Tjmax = 175\n",
    "\n",
    "for i in range(n_iter):\n",
    "    print(f\"Start {i+1}th learning iteration:\")\n",
    "\n",
    "    # 1-GP model training and predicting\n",
    "    if i == 0:\n",
    "        model = fit(X_train_scaled, y_train, n_restarts=20, verbose=False)\n",
    "\n",
    "    else:\n",
    "        # model = fit(X_train_scaled, y_train, n_restarts=20, verbose=False)\n",
    "        model = fit(X_train_scaled, y_train, n_restarts=5, init_lengthscales=init_lengthscales, \n",
    "                    init_variance=init_variance, verbose=False)\n",
    "\n",
    "    # 2-Check fitting results\n",
    "    f_mean, _ = model.predict_f(scaler.transform(X_test), full_cov=False)\n",
    "    has_nan = tf.reduce_any(tf.math.is_nan(f_mean)).numpy()\n",
    "    counter = 0\n",
    "    while has_nan:\n",
    "        print(f\"Bad fitting. Refit the data:\")\n",
    "        counter += 1\n",
    "        model = fit(X_train_scaled, y_train, n_restarts=5, verbose=False)\n",
    "        f_mean, _ = model.predict_f(scaler.transform(X_test), full_cov=False)\n",
    "        has_nan = tf.reduce_any(tf.math.is_nan(f_mean)).numpy()\n",
    "\n",
    "        if counter > 4:\n",
    "            print(f\"Fallback to parameters from last iteration:\")\n",
    "            model = fit(X, y, n_restarts=1, init_lengthscales=init_lengthscales.flatten(), \n",
    "                        init_variance=init_variance, trainable=False, verbose=False)\n",
    "            f_mean, _ = model.predict_f(scaler.transform(X_test), full_cov=False)\n",
    "            has_nan = tf.reduce_any(tf.math.is_nan(f_mean)).numpy()\n",
    "\n",
    "    print(f\"Good fitting. Proceed:\")\n",
    "\n",
    "    # 3-Model assessment\n",
    "    brier_score = confidence_assessment(model, scaler.transform(X_test), y_test, Tjmax)\n",
    "    test_brier_scores.append(brier_score)\n",
    "\n",
    "    # 4-Acquisition\n",
    "    X_pool_scaled = scaler.transform(X_pool)\n",
    "    U_values, indices = acquisition(model, X_pool_scaled, limit_state_value=175, batch_mode=True, batch_size=10)\n",
    "    target = np.min(U_values[indices])\n",
    "    U_hist.append(target)\n",
    "    print(f\"Iter {i+1}: test brier score==>{brier_score:.5f}/{0.01039}, U==>{target:.4f}/1.65, index==>{indices}\")\n",
    "\n",
    "    if target >= 2:\n",
    "        break\n",
    "\n",
    "    # 5-Updating\n",
    "    X_train = np.vstack((X_train, X_pool[indices]))\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    y_train = np.append(y_train, y_pool[indices])\n",
    "\n",
    "    # Update pool\n",
    "    X_pool = np.delete(X_pool, obj=indices, axis=0)\n",
    "    y_pool = np.delete(y_pool, obj=indices, axis=0)\n",
    "\n",
    "    # Update initial guess\n",
    "    init_lengthscales = model.kernel.lengthscales.numpy().reshape(1, -1)\n",
    "    init_variance = model.kernel.variance.numpy().flatten()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6d3249a-ee05-4fb1-b764-8f6fb6e67bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import pickle\n",
    "with open('AL_model_params.pickle', 'wb') as handle:\n",
    "    pickle.dump(gpflow.utilities.parameter_dict(model), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save training data\n",
    "np.save('AL_X_train.npy', X_train)\n",
    "np.save('AL_y_train.npy', y_train)\n",
    "\n",
    "# Save history\n",
    "df = pd.DataFrame({\"U\": np.array(U_hist), \"brier_scores\": np.array(test_brier_scores)})\n",
    "df['benchmark'] = 0.01039\n",
    "df.to_csv(\"AL_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfdc6d8-19fe-41d1-8a71-e7f09771fadf",
   "metadata": {},
   "source": [
    "#### Propose solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82ff05a9-e256-41c9-b0a1-7b4f94f9889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCandidate pool: 470010\n"
     ]
    }
   ],
   "source": [
    "df_candidates = pd.read_csv('./Dataset/candidates.csv')\n",
    "df_candidates.columns = ['d', 'b', 'L', 'c', 'L_duct', 'n', 't', 'xc1', 'yc1', 'xc2', 'yc2']\n",
    "print(f\"PCandidate pool: {df_candidates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3439ccb-e2ec-43a3-a673-4ac3abcc4050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_weight(X):\n",
    "    # Properties\n",
    "    density_Al = 2700\n",
    "    Fan_height = 40e-3\n",
    "    Fan_Weight = 50.8e-3\n",
    "    N_fan = np.ceil(X[:, 3] / Fan_height)\n",
    "\n",
    "    # Weight calculation\n",
    "    w = density_Al*(X[:, 3]*X[:, 2]*X[:, 4]+X[:, 7]*(X[:, 5]*X[:, 8]*X[:, 4]))+ Fan_Weight*N_fan\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eebc86f7-c9e7-4733-a132-4982dd7f1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_df = pd.read_csv('./Dataset/Q_test_locations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "623be34c-abcf-4c91-9f29-f17469b20900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling 1th condition:\n",
      "Handling 2th condition:\n",
      "Handling 3th condition:\n",
      "Handling 4th condition:\n",
      "Handling 5th condition:\n",
      "Handling 6th condition:\n",
      "Handling 7th condition:\n",
      "Handling 8th condition:\n",
      "Handling 9th condition:\n",
      "Handling 10th condition:\n",
      "Handling 11th condition:\n",
      "Handling 12th condition:\n",
      "Handling 13th condition:\n",
      "Handling 14th condition:\n",
      "Handling 15th condition:\n",
      "Handling 16th condition:\n",
      "Handling 17th condition:\n",
      "Handling 18th condition:\n",
      "Handling 19th condition:\n",
      "Handling 20th condition:\n",
      "Handling 21th condition:\n",
      "Handling 22th condition:\n",
      "Handling 23th condition:\n",
      "Handling 24th condition:\n",
      "Handling 25th condition:\n",
      "Handling 26th condition:\n",
      "Handling 27th condition:\n",
      "Handling 28th condition:\n",
      "Handling 29th condition:\n",
      "Handling 30th condition:\n",
      "Handling 31th condition:\n",
      "Handling 32th condition:\n",
      "Handling 33th condition:\n",
      "Handling 34th condition:\n",
      "Handling 35th condition:\n",
      "Handling 36th condition:\n",
      "Handling 37th condition:\n",
      "Handling 38th condition:\n",
      "Handling 39th condition:\n",
      "Handling 40th condition:\n",
      "Handling 41th condition:\n",
      "Handling 42th condition:\n",
      "Handling 43th condition:\n",
      "Handling 44th condition:\n",
      "Handling 45th condition:\n",
      "Handling 46th condition:\n",
      "Handling 47th condition:\n",
      "Handling 48th condition:\n",
      "Handling 49th condition:\n",
      "Handling 50th condition:\n"
     ]
    }
   ],
   "source": [
    "for i, (Q1, Q2) in enumerate(zip(Q_df['Q1'].to_numpy(), Q_df['Q2'].to_numpy())):\n",
    "\n",
    "    print(f\"Handling {i+1}th condition:\")\n",
    "    \n",
    "    # Compile feature samples\n",
    "    Q1_array, Q2_array = Q1*np.ones((df_candidates.shape[0], 1)), Q2*np.ones((df_candidates.shape[0], 1))\n",
    "    X_candidates = df_candidates.to_numpy()\n",
    "    X_candidates = np.hstack((Q1_array, Q2_array, X_candidates))\n",
    "    X_candidates_scaled = scaler.transform(X_candidates)\n",
    "\n",
    "    # GP prediction\n",
    "    f_mean, f_var = model.predict_f(X_candidates_scaled, full_cov=False)\n",
    "    f_mean = f_mean.numpy().flatten()\n",
    "    f_var = f_var.numpy().flatten()\n",
    "\n",
    "    # Utility\n",
    "    Tjmax = 175\n",
    "    likelihood = norm.cdf(Tjmax, loc=f_mean, scale=np.sqrt(f_var))\n",
    "    w = evaluate_weight(X_candidates)\n",
    "    utility = likelihood*1/w\n",
    "\n",
    "    # Sort candidates\n",
    "    df = pd.DataFrame(X_candidates)\n",
    "    df.columns = ['Q1', 'Q2', 'd', 'b', 'L', 'c', 'L_duct', 'n', 't', 'xc1', 'yc1', 'xc2', 'yc2']\n",
    "    df['weight'] = w\n",
    "    df['pred_T'] = f_mean\n",
    "    df['utility'] = utility\n",
    "    df_sorted = df.sort_values(by='utility', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Output results\n",
    "    df_reduced = df_sorted.iloc[:20, :].reset_index(drop=True)\n",
    "    df_reduced.to_csv(f\"Exp_{i+1}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a585b1f-78a5-4e02-8b64-a59777b9be1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
